{"pages":[{"title":"About","text":"The motivation I am an economist by training, that was somehow drawn into Data Science. Since the field is a multidisciplinary one, most people landing on it, initially do miss some of the pieces of the puzzle. I have tried different ways to fill in the gaps: formal training, personal study, online courses, summer school (actually the best experience was in BigDive. Yet I was finding difficult to get all the details right and get a project from A to Z. No wonder since the field is so varied. So, this project grew out of the personal need to get all the pieces together. The main way to learn something is to practice it; and that's what I did. Once you do that though, the next best thing is to explain to someone else what you just did . The objectives You can think of this blog as a free online course with 3 objectives: Present a unifying framework; a necessary starting point in a multidisciplinary field Provide enough knowledge from the various fields, so that a person can pull through with Data Science project. At the same time let people know where they should look at for the things that they don't know Create a production line made of software that processes data and produces insights. One should then be able to plug this production line and use it in any new project One can find all the code and the documentation for the projects presented here, at GitHub I would be particularly glad to receive feedback for the blog. Get in contact with me here: andreas.masaoutis@protonmail.ch Enjoy! Back to top","tags":"pages","url":"/pages/About.html","loc":"/pages/About.html"},{"title":"﻿Introduction","text":"Next article: On Data Science The motivation I am an economist by training, that was somehow drawn into Data Science. Since the field is a multidisciplinary one, most people landing on it, initially do miss some of the pieces of the puzzle. I have tried different ways to fill in the gaps: formal training, personal study, online courses, summer school (actually the best experience was in BigDive in Turin). Yet I was finding it difficult to get all the details right and get a project from A to Z. No wonder since the field is so varied. So, this project grew out of the personal need to get all the pieces together. The main way to learn something is to practice it; and that's what I did. Once you do that though, the next best thing is to explain to someone else what you just did. And here we are... The objectives You can think of this blog as a free online course with 3 objectives: Present a unifying framework; a necessary starting point in a multidisciplinary field. Provide enough knowledge from the various fields, so that a person can pull through with Data Science project. At the same time let people know where they should look at for the things that they don't know Create a production line made of software that processes data and produces insights. One should then be able to plug this production line and use it in any new project The structure of this blog When faced with the problem of conveying a complex concept, one solution is to proceed from the more abstract and simplified to the more concrete and contrived, by gradually adding the complex details. This is what we do here. There are different 'levels' that correspond to different level of complexity. We start with a simple description of what a data project is. We then proceed with a complete example, adding the necessary concepts along the way. Once we are done, we dedicate a level to the increase of technical sophistication and we introduce a lot of the technical tools for a complex project. Finally, we weave everything together, in a complete project. Level 0 - Introduction (You are here): a short intro to get you started Level 1 - Project Description : Setting up and explaining the sections of a Data Science project. We have divided such a project into five distinct phases, and at this level we provide an extended introduction, plus the motivation for each phase. In between Level 2 articles we have placed short tutorials on important concepts: Statistics (Descriptive and Inferential), Linear Algebra, Python and the relevant libraries, Databases, Machine Learning. We present them in order to get a better understanding of the project itself. Level 2 - a Code Example: There, we continue with the implementation. At this level we proceed with a complete mini project that goes all the way to producing a simple project report at the end and an extensive code base. We are fleshing out an implementation of the ideas presented previously, complete with all the code. Intermezzo - the Tools: After having completed a simple project, we add another layer of complexity; more advanced tools that extend our capabilities: Docker, Jupyter, Git, Make, etc. Level 3 – a Pipeline: We revisit our simple project and execute it again, by using the more advanced tools, that give us more capabilities – automation of tasks, tackling bigger datasets, etc. Back to top","tags":"0 Introduction","url":"/introduction.html","loc":"/introduction.html"},{"title":"﻿On Data Science","text":"Previous article: Introduction Next article: Project description-Project start A first look at Data Science To begin with, Data Science is an applied multidisciplinary field. The usual Venn diagram depicts Data Science as the cross-section between Mathematics, Computer Science, and a specific domain - think Biology. This is correct in the sense that the individual parts come from different disciplines. Yet, there is an important element missing from the diagram. What usually drives the process is a specific need. This is especially important in business settings where, as in other engineering fields, there are many technical ways to achieve an objective, but not all make sense from an economic point of view. If we then have to provide a short definition of what we mean by Data Science that would be: reason with data, using the computer as a tool, in new ways, without loosing focus of the practical relevance for such a project How to draw conclusions with data: inductive reasoning As it is of-course the case, it is not the first time in history that scientists use data in order to draw conclusions. Statistics as a field of applied mathematics has been doing so for the last century. At the highest level of abstraction inference with data entails certain steps: Frame an initial question - problem the data can help scrutinise and plan the next steps Decide what the relevant data is; get and store it Clean, validate and summarize it Draw conclusions using the correct techniques Communicate the results and get to use them Reiterate until necessary At this level of abstraction, Statistics and Data Science are indistinguishable. How the digital computer created the field. Without falling for the fad that Data Science is the new promise land we should clarify how the field came to be. In the centre of the stage we find the digital computer and its development during the last decades. There are, at least, two main and interrelated ways in which the computer brought about the recent changes in the way we use and reason with data: The ubiquity of data. As computer chips become part of most of the devices we use, the Internet of Things is around the corner, data on all sorts of things are being constantly produced. The computing power. As computer chips become more powerful, it becomes possible to implement new algorithms, and thus new ways to draw conclusions from data; we will talk more about that in the next sections. A methodology for data science projects There are a couple of important references on the methodology of data science projects. These approaches try to underline the common characteristics of any such project, despite the inevitable differences among individual projects. The most well known, although as it seems somehow dated, is the Cross-industry standard process for data mining , or simply Crisp-DM. A more recent one is Microsoft's Team Data Science Process . Without pondering too much on their respective strengths and weaknesses we will make a couple of important points that will guide our exposition in the following articles. There are certain standard aspects such as, the business understanding, the data and its manipulation, the modelling, and the deployment – use of the results. A data project is an iterative process. It is unlike a bridge, or a piece of software, that might or should be constructed in a linear way. Data projects are much more of an exploratory nature. So what does it take to pull through with such a project? One should be armed with the broad methodology, the specific concepts from the adjacent fields, along with the practical tools. We will cover all these in the next articles. Please note that in actual practice there are other important dimensions in a data project. In order to simplify our exposition we do not pay attention to legal issues, time management, hardware or software requirements. We assume these away, but keep them in mind. sources: Crisp-DM Microsoft's Team Data Science Process Guerrilla Analytics Back to top","tags":"0 Introduction","url":"/on-data-science.html","loc":"/on-data-science.html"},{"title":"﻿Project Description - 1 initial understanding","text":"Previous article: On Data Science Next article: Project description - Get and store data What this section is about This is the setting stage for the project; it's where the foundations are being put in place. As it is the case with most projects, this stage seems relatively easy with respect the heavy work that lies ahead, yet still, a mistake here costs a lot to fix later. Here we have our first contact with the project. In effect what we are dealing here with is a triangle, where each vertex corresponds to the data, the questions we have, and the underlying processes that produce the data and beg the questions. Let us see in greater detail. Data is at the heart of data science, yet data do not spring out of thin air. There are certain processes that produce this data. These processes might take place in far away stars, or within a financial institution; it doesn't make much of difference. The point is that without knowing anything about the process that produced the data, one cannot make sense of it. Even if the data comes with a detailed description of it, still it is difficult to make sense of it. Think of a data set from an ATM: customer_ID, date and time, transaction type, amount, etc. When we examine the data we observe that the amounts withdrawn go up to a certain limit. We could assume that the data comes from an ATM, but what if there is a bank run in this country and the limits also apply to over the counter transactions? As you can see, data alone is almost useless, without knowledge of the processes that produced them. That is why is aim at collecting the knowledge that lies behind the data. At the same time, although the technical questions we try to solve in data science are pretty clear-cut, for example trying to predict the class of a new event, it is not that straight forward how to arrive at that final formulation of the question. Usually projects start with a more vague understanding of what is there to achieve. Keeping in mind the processes that generate the data and the goals of the stakeholders is necessary in order not to go astray. As we will work with the data on commercial passenger flights in the US, think of a scenario where we are being given a data set of all flights in the US, by a start-up that wants to build an application that will be predicting flight delays. Now, suppose that on the first run we see that we cannot deliver a really good prediction of delays. Rather, we can either predict delays on a subset of all flights, or we can provide insights on the delays but after these have taken place. Which way should we move forward? Well, this is a business question. Maybe the start-up should move into consulting airlines, rather than informing passengers. In any case, the decision comes from the business specific domain. To keep it short we can make these points: knowledge on the background processes shed light on the nature of the data. Understanding the data, improves our understanding of the background processes. The background processes, or the business setting, is necessary in order to understand the question at hand and define the ways to evaluate success or failure. Obviously the data will be used to shed light on the question Yet, the question itself provides some rough guidelines for approaching the data – decide on the relevant variables, etc In two words, the objective in this section is to define and describe the triangle of data, background, and question, in the best way possible. Input As we have been pointing out, there are certain similarities between Data Science and other fields. In this case, in software engineering we begin with requirements gathering; in academic projects with a research proposal and an initial secondary literature. The input for the initial understanding of a data science project will be somewhat similar, too. This is a list with some commonplace items to look for: One should be able to capture the stakeholders' view of the project. What is important to them, their objectives, etc. This can be captured through interviews, questionnaires, etc. As we've pointed out already, the initial question is usually embedded in a domain. Here, we look for the domain background of field/industry/problem. One aspect is the domain description: eventually the processes that generate the data. The other aspect is the \"literature review\": what others have written and discovered about the problem domain. An overview of the possible sources of data, that might be relevant to the project. We should have an inventory of the material we have found relevant. Processes Output The main objective is to capture the triangle of data, background, and initial question. The desired output is a good description of this triangle. This is a list with some commonplace items to look for: We need a summary that distills the stakeholders' view from the input documents produced above, and provides the background for the main question, or family of questions. What are the assumptions behind these, and how do they relate to the aims of the project, the ways to assess success, etc.. Ideally we should have framed a hypothesis, that should guide the next sections. A description of the the background/domain. These are the real processes that explain what the relevant data is, and also the canvas on which the stakeholders operate. We need to have an understanding of these so that we can shed light on both the stakeholders' views and the data A decision for the data: what would the relevant data be with respect the initial question and background. Example Let us provide some context for the things we have presented up to here. This is a data competition from 2009. The data consists of flight arrival and departure details for all commercial flights within the USA, from October 1987 to April 2008. The challenge posed, along the dataset, a list of possible questions the participants could address, such as the role of weather on flight delays, etc. So, we have a challenge. The ASA gave us a data set and a list of possible questions. Other people before us have participated in the competition and provided us with their findings. We could read these findings, go through the supportive material provided by ASA, put together our knowledge of the passenger airline industry. We could have a project started this way. Actually, once we are done with the project description we will use this challenge as our testing ground. For now, it is enough to see what a project looks like initially. Attention If we have to pay attention to a couple of points these would be: The business/domain is the canvas for the main question, and determines whether the question holds value for the business or not. One of the pitfalls for technically competent, yet careless data scientists, is to provide a great answer to an irrelevant question. Resist the hurry to jump into crunching data. The domain canvas, not only separates what is valuable from what is not, it is foundation for the whole project. Ask as many questions as possible, be as explicit with the assumptions as possible, and be systematic with what you collect. It seems like slowing you down, but it will later, surely, pay back. Back to top","tags":"1 Project Description","url":"/project-description-1-initial-understanding.html","loc":"/project-description-1-initial-understanding.html"},{"title":"﻿Project Description - 2 get data","text":"Previous article: Project description - Project start Next article: Project description - Data wrangling What this section is about Once we have an initial understanding of the domain and the question at hand, it is time to move forward towards our data. In this section we have our first contact with the actual data. Data might come from different resources and in different formats. What we have to do is: to build our collection of data, decide about the ways we are going to store it, ie. the data architecture, and once stored, verify that the data is syntactically correct – ie. the structure of the data is correct. Let us see in more detail. They may come as simple files in various data formats: csv json a spreadsheet file simple unstructured text and many others. Sometimes, we collect the data ourselves by scraping the web. It could also be the case that instead of individual files, data might reside in a database, in which case the relevant data has to be queried. We will cover more on the databases section. Overall, the point is that we have to collect the data that might even come in different formats. Yet, simply accumulating files with data, is not good enough; we need to homogenise and store them in some way. Think for a moment that we are being given a collection of files in different formats. Every time we need to get a set of data we have to go through many files, find the relevant pieces, bring them together, and transform them into a common format that is suitable for us. It is obvious that, although doable, such practice will not be productive at all. Instead, we should think once about the architecture of our data, implement a design, and have a homogeneous source of data that we can use. If, for example, we think that we might by using a computer language that has much better support for json rather than xml, then it makes sense to bring everything into json. There might be reasons to create one big master file with all the data, but if that gets too big for our file system or RAM maybe we should keep them split apart along some lines that make sense for our project. If we decide to keep the data in a database, the question arises: what kind of? Relational ones are the most common. But if we know that we want to perform some kind of network analysis, then maybe we should use a network database that is specific for the task. Without going any further it becomes clear that we have to think good about how to get, homogenise, and store our data, in such a way that it will be available and easily accessible to us during the project. In the process of doing so, we have to address an issue that relates to data quality. When we collect data, make transformations, and try to store them into a format/means, there is ample room for problems. Date formats is among the most common problems. Think of a simple case: same hardware, same operating system, same spreadsheet program, same data set, yet from one point in time to another the data analysis produced different results. Weird? Not at all… What had happened was a change in the system locale, that changed the way dates were interpreted. In the end, this is something trivial yet exactly because of that, in practice it can be really time wasting. And make no mistake; there are many little things that can go wrong: a trailing comma in a csv file that messes up the columns, a different specification in handling strings (single quotes, double quotes, etc.) that render inconsistent results, different end of line escape characters between operating systems, and so on. In order to address the problem we should immediately make sure that the data we have stored in our chosen architecture are in accordance with the data sources and with the specifications that we might have on that data. In order to do so we can perform various checks. After, for example, we input records into a database we should verify that at least the types and ranges of the various columns are correct. This step can be performed without knowing much about the data itself. But we definitely tell whether a column that is supposed to hold integers or dates does indeed hold these. But we can get only so far, since properly cleaning the data means looking at it and understanding it, which will be done in the next stage. input At this stage, the input is all the various data sources that have been identified previously, and finally chosen here. processing steps: Collect initial data Decide on the data architecture and store it Describe data Verify conformity to data standard - syntactic test output review of decisions on data architecture, software choices, etc. the \"read only\" stored data - database, csv files, etc. code scripts for getting, loading, and verifying data data description - types, ranges, etc. example We continue with the airdata example we introduced in the previous article. The competition organisers provided us with the relevant data. There is the main flights data section with one csv file for each year from 1987 to 2008, as well as supplemental data with csv files with details on airports, aircrafts, etc. So we have our source of data, and we should make a decision about how to store the data. Remember though that the decision cannot be independent from the questions we want to ask and the analysis we plan to perform. If we simply want to focus on a certain year, say we are interested in a major snowstorm, we can maybe simply keep the csv of the relevant year. In case we want to focus on a certain airport or airline we might go through all csv files, dig out the relevant entries and keep them in a separate file. For more elaborate uses of the data, loading them in a database might be preferable. Depending on the kind of analysis we should choose different database technologies. Whatever we decide, we should take advantage of the documentation provided with the data, and produce our own for the choices we made. We will then be having a good source of data to begin with. ##attention If we have to pay attention to a couple of points these would be: Be careful with the details. A trivial detail can create a lot of problems with your data and be hard to detect. Spend some time to think about the architecture of the data, thinking about the possible use scenarios. If you don't get it right, it might be time consuming to get back and rebuild the data, even more if in the meantime there is a lot of undocumented choices about the data; if you change the source a lot of things will be then broken. Back to top","tags":"1 Project Description","url":"/project-description-2-get-data.html","loc":"/project-description-2-get-data.html"},{"title":"﻿Project Description - 3 clean data","text":"Previous article: Project description - Get and store data Next article: Project description - Drawing conclusions with data What this section is about After having decided about the data we should collect and store, in this stage we start to prepare the data for the subsequent analysis. There two important goals here. First, to avoid what goes by the acronym GIGO; that is ‘Garbage In, Garbage Out'. It is obvious that if the values that we use are not correct, then either the analysis will not produce results, or even worse, will produce statistically valid results, but that will fail to materialise in reality. Second, to get a deeper idea about the variables in the dataset, and their interrelations. Therefore, what we have here are three main goals, where each one of them informs the others. As it was the case in stage 1, here too we have a triangle where each vertex is cleaning, validating, and summarising our data. Let us see in more detail. In the previous stage, we tried to make sure that the data we have conform to some rules. If, for example, we loaded records in a database, we wanted to make sure that each column has the correct type. Here we can take a step further. If a field goes under the name ‘patient height' we should have already made sure that each record is a float number – measuring in meters. Suppose we observe the value 18.5m. In that case it is almost sure that the decimal sign is misplaced by one digit, and we should correct this value. Another example is a misspelled string, say a company name, which would then appear as a separate entity, although it's not. This should be fixed too. This is the sort of activity that I use the term cleaning for. If we do all that - we will be more explicit in the next level - we will have a caught many data glitches, input errors, etc. But we will still need something more; we will have to compare our data, against the domain canvas. This is necessary since we cannot interpret data otherwise. Revisiting the airdata example, suppose we have a flight record with normal fields. The departure time is before the arrival time, the dates are ok, etc. We perform a calculation to find the speed of the flight (distance over airtime), and surprisingly it is higher than 1 Mach. Is this possible for a domestic passenger flight? Well, in order to clarify the issue we need to get further than looking at the data set and delve into the specific domain. Only in this way we can validate that what is there in our dataset, corresponds to the real processes that produced it. As you have probably noticed, we have already made use of summary statistics. In detecting the abnormal ‘patient height' we didn't check every individual record, but looked at the range of the values which showed the abnormal maximum. That is, summarising the main characteristics of the dataset, is not something to be done in the end. It is part of iterative process. Actually, we could even start with producing summary statistics and plots for the data. This first look will help us to detect possible anomalies to be further examined, that will lead to a better understanding, that will lead to better summary statistics, and so on. input At this stage the input is the main data source we have prepared previously. Note, that the domain knowledge collected in stage 1, will also be useful in make sense of the data. processing steps First effort to describe data - summary statistics, lots of plots. Flag possible errors and outliers Perform semantic tests against the domain canvas Decide what should be \"flagged\". Removing records might not be good practice, flag them as inappropriate instead. Produce summary statistics and plots. If more problems spring out, repeat the above processes. output After having performed the above steps, we should be having these: Review of decisions on data cleaning: outliers, input errors, etc. Data description report. The cleaned data set; a transformed copy of the \"read only\" raw data. The code script for that section, that should be reading the data, fixing what is to be fixed, and producing the necessary plots and summary statistics tables to be used in the data description report. example We continue with the airdata example we introduced in the previous articles. In the previous stage we have decided on the way to store the data relevant for answering the questions at hand. It is now time to start exploring the dataset. How many airports and airlines are there? The variable descriptions of the flights data indicate that departure and arrival times are recorded in local time. Do these make sense? Are these in accordance with the flight duration captured in other variables? Do different air carriers have variations in the delays? In the next section we will see some of our findings. attention If we have to pay attention to a couple of points these would be: Instead of removing records, flag them and leave them with the rest of the data if it is possible. You might remove something that shouldn't have been removed. Also, the ‘bad' themselves might have certain patterns worth investigating. Use a lot of plots. Visual inspection can provide more information, faster. Don't forget the domain. You need it to understand what is normal and what is not. sources: Tamraparni, Johnson, Exploratory Data Mining and Data Cleaning McCallum, Bad Data Handbook Squire, Clean Data Back to top","tags":"1 Project Description","url":"/project-description-3-clean-data.html","loc":"/project-description-3-clean-data.html"},{"title":"﻿Project Description - 4 drawing conclusions","text":"Previous article: Project description - Data wrangling Next article: Project description - Deployment What this section is about This is the heart of any project that works with data. It is where the question will meet the data, resulting in a conclusion. It is the most, technically, complicated and difficult part, and the same time the most rewarding, if you get it right. It is where statistics, and linear algebra, and computer science, meet in the field of Machine Learning. For the time being we will provide a simple description, enough so that the methodology makes sense. In the previous stage we created a clean dataset, and got a deeper understanding of the variables and their relations. We also have an initial question/objective. The way to connect these two ends goes through a series of steps that pass through four layers: First of all we have our question/domain problem. We revisit the start-up that wants to build an application that can predict whether a flight will be delayed. This problem translates, in the language of Machine Learning, to a problem of classification. Given an observation, a certain flight with all its characteristics, we should assign it to a class along with other delayed flights or to the class with the on-time flights. There many different techniques to classify, each with its own characteristics. We can choose a decision tree for their easy and generality. Once we have chosen a general modelling technique we have to decide on the specific algorithm that will be used to implement the said technique. In the case of decision trees that could be CART Deciding on the modelling technique and the specific algorithm depends heavily on the properties and the characteristics of our data. Certain algorithms do not handle well categorical, while others do better. Some struggle with with noisy data while others do not. These observations brings us to the important lesson in Machine Learning that no algorithm and model is always better than the rest. Every time there is the right tool for the job. This lesson comes under the name, \"There is no free lunch\" Once we have decided on the algorithm to be used, it is time to revisit our data. They are clean but not yet ready. First of all we should create new features that might be relevant for our model. In the case of the airdata, if we want to include the age of the aircraft as an explanatory variable, we will first have to compute it. Second, we should bring the variables in the right format. For example, certain algorithms need that numeric variables are standardised, otherwise variables with larger numbers in absolute terms tend to dominate, and thus invalidate, the results. If it is possible it to would be great to save separately the specific dataset with the new features, so that model and the particular data version go hand in hand. After the relevant features are created, it is time to estimate our model. This is the process of feeding the algorithm with the data, so that the former learns, in this case how to classify, each observation. The tricky part, is that each modelling technique and each algorithm, come with many parameters that control the behaviour of the model. It can be a fairly complicated matter, to get the parameters right. We will cover certain methods in the next sections. Our model is now trained, and it has ‘learned' to predict whether a flight will be delayed or not using, in the case of decision trees, certain rules. Still, we are not there yet. We do not know whether this model is any good or not. It has its accuracy and other metrics but we cannot know how it will behave if we try to use the rules of this model to predict new observations, not seen before. In order to estimate the behaviour of the model in the future, we use a portion of data that has not been used before; the test data. It is on this dataset that we calculate certain metrics, hoping that these will produce an accurate estimation of the model behaviour in the future, with completely new data. A rough scetch to help you decide what kind of algorithms you need, given the task at hand source: scikit-learn documentation input At this stage the input is the clean dataset produced previously. processing steps Start with a hypothesis, such as, by using that and that variables we can predict / explain the desired outcome. Go through the four layers of data mining and select the modelling technique and algorithm. Create the suitable features for this technique according to the initial hypothesis. Split the data in training and test sections Choose the parameters and train the model using the training data Assess it using the test data output The desired output for this section is: The modelling report. It is the main document that describes all the choices and results. A copy of the data set to use for the model A copy of the trained model The code script for this section example We continue with the airdata example we introduced in the previous articles. In the previous section we produced a clean dataset and the report on it. The initial question we had was about the delays and whether we could predict them. The dataset contains a series of variables that could be used as is. Note that there are many different categories of delays. See for example variable 29 that captures the delay due to an aircraft being in delay. That variable could be the basis for a small model about this specific kind of delay. If, in addition, the data exploration has demonstrated that older planes tend to have more delays, we could construct a categorical, or maybe binary, value in order to capture this feature: whether a flight uses an old plane or not. We should also take a look at the presentations of the other participants in the competition, and search for other possibly useful features. Then we should decide on the modelling technique and the specific algorithm. Once we have that, we create the final dataset, with the relevant features in the correct form, and proceed with training and estimating the model. Hopefully, we will have a good prediction and classification at least about this kind of delays. attention If we have to pay attention to a couple of points these would be: There is no best modelling technique and algorithm. Each is best for certain situations and before choosing among alternatives one should have a good understanding of both the problem at hand and the alternatives. Beware of data snooping. The saying that \"if you torture your data long enough, it will confess\" provides us with a hint. It is not difficult to train models that perform well on the training data if you try all conceivable alternatives. These models will not necessarily perform well on completely new data. Back to top","tags":"1 Project Description","url":"/project-description-4-drawing-conclusions.html","loc":"/project-description-4-drawing-conclusions.html"},{"title":"﻿Project Description - 5 communicate","text":"Previous article: Project description - Drawing conclusions with data Next article: Tutorials - Python programming What this section is about This is the exit point for our project, and hopefully a successful one. Te be more precise, this might not be end of the project altogether. Even if we put together the ‘final report', it is final with respect our involvement. The report will be used by the stakeholders to give insight to their actions. It might also be the case that the project lives one as production code for an application. In any case, it's not that we are done and we don't have any responsibility any longer. Others will use our work and the quality of it will make a difference. Until now, we have seen, somehow, a single iteration: from initial question, to various data transformations, to modelling. We are not done in the first try; each pass over this cycle gives us some insight, and we repeat until we get the results we want – or give up... In any case with every iteration it would b good to prepare even a rudimentary set of documents that belong to the final stage. The main objective here, is to gather the knowledge/insights we have found out, present them in a comprehensive, yet clear manner, and help whoever is going to use our report/product to see the choices we have made and the reasons for doing so. In this way they will be able to understand the strengths and limitations of our work, and it should be easier to make changes in the future. If we have been careful to produce a detailed documentation along the way, then we should have most of the pieces necessary for our report; from the domain knowledge and the initial questions, to the detailed data description, to the modelling report. input For this section the input is all the pieces that have been produced up to this point. processes Revisit the various stages and compile the project report. Then, both the executive summary and the presentation can be put together. As for the code base, if the scripts for each stage have been correctly produced, then it should not be any difficult to put everything together. output The desired output for this section is: The project report. This is the main description for the whole project. As we've said, it should comprehensive and clear. It should make obvious the choices made and the pieces of knowledge attained. The executive summary. This is a shorter version of the project report. This is not because executives are stupid; it is rather that their role is to make decisions. Therefore the executive summary focuses more on clarifying possible courses of action. The project presentation. As it is usually the case, there is a final presentation to be used in meetings with the stakeholders. The code base. Even if the project is not handed over to the engineering team for deployment, it is still necessary to produce a clean code base to be handed over. One reason is reproducibility. As with any research activity, ideally, others should be able to independently verify our results. The stakeholders should be sure that we have not just threw some fancy code snippets and visualisations. The other reason is the ability to revisit and extend the project in the future. Maybe a new dataset has been made available. It would be a waste to have to write all the code from the beginning. example attention If we have to pay attention to a couple of points these would be: The narrative is important. One cannot simply throw the technical material on a PDF file and get done. There are many reasons for that. For example, different audiences have different experiences and competences; it can't be one size fits all. The very nature of a data science project is that of putting many different parts together, and the connections and motivations for these connections, are not always immediately visible. The project structure. It is worth repeating that if the project structure is good, both code and reports are well documented, the decisions made with explicit thinking, etc., then it will be easier, not only to get through with the project, but also to communicate it. Visualisation is really useful. It is a cliché to say that a picture is worth thousands words, but in many cases a good visual representation of our work can get us a long way, both in analytical terms and communication. Reproducibility matters. It is suffice to mention the case when two well known economists writing on a hotly debated matter, failed to demonstrate how they arrived at their conclusions; a fact that damaged the validity of their argument, and possibly their reputation too. (See the case of a 2010 economics paper Growth in a Time of Debt ) Back to top","tags":"1 Project Description","url":"/project-description-5-communicate.html","loc":"/project-description-5-communicate.html"},{"title":"Concepts - Linear Algebra","text":"Previous article: Project description - Deployment Next article: Concepts - Statistics Concepts: Linear Algebra To Be Added sources: Back to top","tags":"2 Concepts","url":"/concepts-linear-algebra.html","loc":"/concepts-linear-algebra.html"},{"title":"﻿Concepts - Statistics","text":"Previous article: Concepts - Linear Algebra Next article: Concepts - Machine Learning Statistics: inferences from data As a brief introduction we will try to define what statistics is, and to clarify the relations between statistics and adjacent fields like mathematics and machine learning. Statistics has a main goal and that is to extract meaning from data. Moore uses a definition that refers to the distinct steps of statistics project: 'Statistics is the science of collecting, organizing, and interpreting numerical facts, which we call data'. Let us start with what data is. It is not just numbers or labels; one does not have data without the context. A list of numbers is a list of numbers without the knowledge of their meaning. Without the context we do not know if the numbers make sense or not and we cannot be sure whether we are allowed to use statistical techniques. Since we deal a lot with numbers, mathematics do have an important role; it provides the theory at a level of abstraction characteristic of mathematics. For example, a binomial distribution behaves the same for all random processes that have a binary result; and that is great to know. From the moment we move away from the abstraction towards the real world, we need to make all sort of decisions and use our intuition. That is, we move beyond mathematics, into statistics. As statistics is an applied field it grew out the practical problems of science and industry. The form statistics has assumed in the 20th century can be in great part explained by these needs. Think of quality control for a moment. It is imperative to work with a sample, since we cannot simply try every single barrel of whiskey to verify its quality, and find a way to draw inferences from the sample so that their validity extends to the whole population/production. Out the practice grew the three main areas in statistics when working with data: The collection of it The organisation of it The drawing of inferences from it Let us see each part in more detail Sampling: the standard way to collect data In sampling we do not simply gather data. We try to make sure, as much as possible, that the data we gather are representative of the whole population. Without this precondition no amount of technical sophistication will produce correct results. If, for example, measure people's height outside of a basketball court, we will not get a representative sample of the population. and subsequently fail to estimating the height of the population. describe Once we have our sample, the next thing to do is to use descriptive statistics in order to summarise the data and describe its main features. This is also called exploratory data analysis. It is our effort to a first systematic look at the data. We start first by looking at each variable we have and try to verify that it conforms to the its standard. We go on looking at the possible relationships among the variables. In doing so, we use a lot of plots, but also numerical summaries of the data. That's where some of the mathematical concepts of the probability distributions, and their characteristics, the mean and standard deviation, concepts like correlation, enter the picture. infer The third stage, after having a sample and an initial picture of the data, is drawing inferences. The usual process is to formulate a hypothesis - say that the true mean of the population is zero, compute the relevant statistic from the sample - in that case the sample mean, and then using the theory, try to infer whether the population mean is indeed zero or not. That's the fundamental concept in statistics: compute a fact about the sample, and try to guess the truth about the population. using the computer In the past inferential statistics rested heavily on theoretical notions. With the computer we have developed computational techniques to draw inferences while relaxing the assumptions necessary for the theory to work. We will make a comparison between classical inference and the bootstrap method in order to demonstrate the difference. A central concept in statistics is the sampling distribution. Imagine a population that has a characteristic, say average height. If we draw a random sample from the population, this sample will have an average height, that might or might not be close to the population average height. If, thought, we take many samples from the population and write down the average height for each sample, then these average heights follow a distribution of their own, with a mean, close to the true mean of the whole population. This is the sampling distribution and usually it follows the normal distribution. The sampling distribution is fundamental for taking the next step, to relate the sample to the population. In traditional statistics, we do not know what the true population mean is, and we cannot draw many samples. We only have one. Then we rely on the theory, the central limit theorem, to assume the form of a hypothetical sampling distribution, that we will then use for the practice of inference. There are certain cases though, that the theory will not work. In these cases we are left with only a sample. The computer then comes to the rescue. In order to approximate the sampling distribution we repeatedly draw samples with replacement, from our initial sample. It turns out that the distribution of the bootstrapped samples can be used for inference. This is an example of how computation can substitute for theory. statistics in data science In data science we make use of statistics; directly using ideas in exploratory analysis and the whole machinery of probability distributions and other concepts, and indirectly as a basis for statistical learning - a big part of Machine Learning. Machine Learning is different compared to statistics: 1- Data are not carefully produced, therefore, many of the techniques developed in traditional statistics do not work 2 - With the computer we can manipulate the data in other ways apart from those that fit into a mathematical context. That does not mean statistics is irrelevant to Data Science; on the contrary the lessons learnt in the former can be really helpful with the latter. Think for a moment a situation that does happen with data analysis. In the presence of a big data set we might want to work on a smaller part. If we are not careful enough to draw a random sample, then we will not find interesting patterns that are indeed in the whole data set, and we might 'find' pattern in our sample that are simply non existent in the data set. Using a statistically savvy mindset, can protect from a lot of traps waiting for the unwary data analyst sources: Practical Statistics for Data Scientists: 50 essential concepts Naked Statistics Think stats Introduction to the practice of statistics Back to top","tags":"2 Concepts","url":"/concepts-statistics.html","loc":"/concepts-statistics.html"},{"title":"﻿Concepts - Machine Learning","text":"Previous article: Concepts - Statistics Next article: Concepts - Programming Concepts: Machine Learning ##What is it about Machine Learning is a field that draws on computer science, statistics, and mathematics with the aim of replicating what passes as human learning in specific application domains, such as pattern recognition. In an overly simplifying way we could say that ML is linear algebra + probability theory + statistics + computing. It has spawn off from Artificial Intelligence, when the latter went on in its effort to replicate human intelligence, while Machine Learning focused on solving specific domain problems and relied more heavily on statistics and probability. The digitalisation of greater amounts of information and the cheaper computing power gave ML, a wider field of application, along the material and the capabilities to build models. The ability to replicate specific domain application prompted ML to be adopted in industry in many fields. Where does it fit in Data Science Machine Learning lies at the core of any data application. It provides the machinery for transforming data into insights. We do not mean to say that it is the single most important part of the pipeline; we have seen that with bad data you cannot get good results. And in many situations even understanding the data, and using traditional statistical tool is already a long way. Yet it is the workhorse of any project; if it works right one can get often impressive results. The details In a high level of abstraction we could say that Machine Learning is a form of inductive reasoning, as it also the case with statistics. In broad terms the objective is to draw inferences: If A then B - the machinery might be diverse but the logic is the same. We construct a model that we believe describes/replicates the real world processed We feed the model with data and estimate the parameters of the model Then, we bring new data to the estimated model, and make inferences about the qualities of the new data Let us now see how this generic description takes a specific form, in the case of Machine Learning. The problem representation Most learning problems start with a mathematical representation of the task, along which comes an function that measures the errors the model makes. In the case of linear regression we have a straight line/plane that goes through the data and the error function measures how far our line/plane is from all the data points. In the case of classification/decision trees we have there are branches that represent decision the model makes at each node, while there are different error functions that measure, in a classification setting, how well the model classifies data in the categories created. From representation to optimisation Once we have a way to represent the problem and have defined a way to measure the success or failure through an error function, the next step follows naturally. We try to minimise the error; this is the fundamental way we have te represent learning. In the context of classification trees, that means create the final classes with as few mis-classifications as possible. In this way we make the transition to mathematical optimisation; given a function that represents the problem, choose the parameters so that the function when fed with the data will produce the smallest possible error. What about other possible data? Beware! We have minimised the error for the data we had; what will happen when new, unseen data, will be fed to the model? The new data will not exactly match what we used in the first place, so the error will be different. An important idea here is the bias - variance trade-off. Suppose we can make our model too complex so that it will mimic really close the initial data - that is achieve low bias. But when new data will come in, the error we will get will vary a lot - that is there will be high variance. The alternative is to have a simple model that will not capture the initial data really good - there will be high bias. Yet, when new data come in, the error will not vary too much - the variance will be relatively low. The important lesson here is that we cannot minimise both bias and variance at the same time. We have to find just about the right balance between the two. So, how do we assess our model? When we train a model we can compute the error that the model makes with the data that it was trained. We need a way to estimate the error when new data will be fed to the model and gauge how well the model will behave in the future. There different techniques for doing so, but all depend on the same idea: use the data at hand to simulate the unseen data. One common approach is to split the data into a training set, and a holdout set. In the case of cross-validation, the training set is further split into many random training and test sets. We repeatedly train our model on one set and test it on the other set. This iterative process help us find the model parameter that most probably will produce the lowest possible error with a new data set. Once we are done with the cross-validation, we use the holdout set verify that our results hold even with a new set that has not been used at all. sources: James, Witten, Hastie, Tibshirani An Introduction to Statistical Learning Abu-Mostafa, et. al. Learning from Data Back to top","tags":"2 Concepts","url":"/concepts-machine-learning.html","loc":"/concepts-machine-learning.html"},{"title":"﻿Concepts - Programming","text":"Previous article: Concepts - Machine Lerning Next article: Tools - Python libraries Python programming for data science This is a short tutorial on programming and Python; as such it can not delve into the details. Rather, we would like to focus on some concepts that can help readers understand where do programming and Python fit in the data science field. There are two fundamental ideas of computer science, that we should mention because they provide the basis for much of what we use nowadays. First, the Turing Machine. An abstract machine that can perform any conceivable calculation. Second, the idea that we can represent logic functions by using binary gates, first made of vacuum tubes, now made of silicon. These two ideas together mean that we have an abstract model of what can be computed, and a way to build a concrete implementation of the abstract model. Then the digital computer was being born. Computer languages, give the ability to implement a model of the Turing Machine, while the computer hardware performs the actual computations. That is why, most computer languages, share a core of characteristics: There is a way to receive input and provide output There is a way to store things in the computer memory There is a way to read things from the computer memory There is a way to perform arithmetic manipulations, such addition, division, etc. There is a way to implement iterations where certain actions are repeated There is a way to implement execution of actions depending on conditions And that's it more or less... Of course there is object oriented programming, networking, and so many more things, yet the core capabilities of any language come through this set. Python is a relatively new language. One of the main advantages it has for newcomers is the readability of the code, and its simplicity. It is therefore much easier to learn, and it provides similar functionality. Being open source with a significant ecosystem of libraries that extend its functionality, it has caught pretty well in the field of data analysis. There is one point that is worth raising with respect a technical difference between python as a generic programming language, and when being used data analysis; array programming . It is programming paradigm much like functional, object oriented, etc. What is the difference here? In scientific computing we usually perform calculations over whole arrays of elements, rather than individual scalars. The reason being that in the end it is much faster to work on whole arrays and at the same time it is closer to the mathematical concepts. This functionality in Python comes through the NumPy library. We stress this point because the way we write code in data science, is different than that in other fields. You will not find loops or conditionals when crunching data, although these are so common language constructs, much used in other paradigms. So if you are new to programming, pay attention, and don't get confused by the different programming paradigms. Pay attention to the different Python versions. There are two main versions: Python 2.x and Python 3.x. Your system might have one of the two installed, while the material (books, online tutorials, etc. ) you find, might be using another. Beware that Python 2.x has reached its end-of-life, with 2.7.18 released in April 2020. sources: Software Carpentry - Python programming lesson Think Python Python for Everybody: Exploring Data In Python 3 Back to top","tags":"2 Concepts","url":"/concepts-programming.html","loc":"/concepts-programming.html"},{"title":"﻿Tools - Python libraries","text":"Previous article: Concepts - Programming Next article: Tools - Jupyter Python libraries for data science In a previous post we have talked about Python. One of its main characteristics is what the Python community calls 'batteries included' design philosophy. Python not only is a complete language, but can be extended with a vast variety of libraries that allow the user to do virtually anything. The standard way to use these libraries is to install them in the system along the Python installation and then import the library in the program we need it. One important motto that one will come across is 'Read the Docs'; actually the motto has become the name of the homonymous software documentation platform. In the spirit of the Python ecosystem, the libraries we present, do have good documentation that is worth reading. To be precise, one should read the docs before using the library and will be helpful to keep the reference handy along the way. NumPy The name, in a sense, stands for Numerical Python. Since the language was not initially intended for scientific computing, it lacked some relevant features. NumPy is built around a data structure, the 'ndarray', upon which rest the other libraries of the scientific stack. In contrast to other Python built-in types of arrays, such as the list and the tuple, an 'ndarray' contains elements of the same type, and can be used to represent data structures of more than one dimensions (n-dimensional array). In this way we can represent mathematical structures such as matrices. Additionally it is possible to introduce vectorised operations that run much faster. Instead of using standard Python loop structures to perform an operation on an array, say multiplication with a scalar, NumPy can perform the operation immediately - still there are loops running in the background but much faster since the C language is being used over homogeneous input. matplotlib This is the main 2D plotting library for the whole of the Python ecosystem. It can be extended to cover 3D plots, and there are other libraries like bokeh and seaborn that build on top of it. It aims at being easy to create plots yet at the same time customisable to user needs, therefore it contains a lot of standard plots that one can easily call and pass data to them, while at the same time all the elements of the plot (legend, labels, markers, etc.) can be changed plotly Add interactive graphs pandas The main scope of this library is data manipulation and analysis. pandas builds on NumPy and the ndarray type. It provides two main types of its own the Series and the DataFrame, although under the hood the basis is the ndarray. Series is a one dimension homogeneously-typed array, while a DataFrame is a 2D collection of Series that looks like a spreadsheet. What the DataFrame brings is the fact that is labeled, columns have names that is easy to access and change. pandas then creates a whole assortment of methods to operate on the Series and DataFrame, that represent the common data manipulation actions taken by the data analyst; slicing the data, changing the index, group by and then perform operations on the groups, along with many more. pandas also allows to draw plots and perform data analysis tasks. scikit-learn This is the main machine learning library that can be used with the rest of the ecosystem including NumPy and pandas. It contains various classification, regression and clustering algorithms that create the core of scikit-learn. It is build in order to implement the work-flow of a machine learning task: choose the model and its parameters, create the training and test datasets, train the data, and assess the model. One can additionally perform feature engineering tasks, like standardisation, or use elaborate algorithms for that effect, like PCA, etc. All in all, everything one needs for machine learning NLTK Natural language processing virtual environment Create a sand box for the project. sources: NumPy manual Matplotlib User's Guide Plotly Software Carpentry - Plotting lesson pandas documentation scikit-learn documentation [nltk]: http://nltk.org/ Back to top","tags":"3 Tools","url":"/tools-python-libraries.html","loc":"/tools-python-libraries.html"},{"title":"Tools - Jupyter","text":"Previous article: Tools - Python libraries Next article: Tools - Databases Jupyter To Be Added sources: Back to top","tags":"3 Tools","url":"/tools-jupyter.html","loc":"/tools-jupyter.html"},{"title":"﻿Tools - Databases","text":"Previous article: Tools - Jupyter Next article: Worked Example - 1 initial questions Tools: Databases What is it about Databases provide us with a way to store data and manipulate it. The acronym CRUD, which stands for Create, Read, Update, and Delete, describes what we can do with the data. Although we use the term database, as if it was a singular entity, simplifying a lot, we can say that a database system is comprised of two parts: the physical means, such as a file, where the data is stored, and the program that allows the users to interact with the data. These systems were invented in order to overcome the limitations of using simple flat files to store information, as it happened until the 70's. Database systems can provide important functionality and solve practical problems for mission critical applications, such as banking transactions , complex project management, etc. There are different kinds of databases, that one might come across in data projects, each with its own characteristics. By 'different kinds' we mean, that they are built around different concepts of how to model and store data, along with different ways to interact with it. These differences lead to database tools that are suitable for different uses. The well established relational database was on of the first such systems. It relies heavily on a theoretical model of data, developed in the early 70's. The majority of the databases from the era of the mainframe computers to that of the personal computer, were of this kind. A then new computer language - SQL (Structure Query Language ) - was invented to program with these databases With the advent of the Internet, the industry developed new tools to cover new needs. Relational databases, cannot be easily distributed across many nodes, and become cumbersome to use after they grow in size. With the era of big data the so called, NoSQL databases made their appearance. Document databases are much more flexible in their theoretical model about data. As a result they can handle more complex kinds of data, such as text, with less rigid rules. In addition they can be distributed, so by adding commodity hardware in a network one can store and work with any amount of data. Network databases focus on the representation of real world networks, in such a way so that it is easy to insert the data, but also query it, and work with the network. As time goes on and technology advances more NoSQL databases will become available. How does it fit in Data Science In Data Science databases play a significant role. As projects grow larger and work with bigger amounts of data, it is quite common for the data to reside in a database system. With the functionality of databases it is possible to perform most of the parts of a data project inside a database, like as we saw clean, validate and summarise data. In some cases, when the data does not fit in the RAM memory of the computer we use, we do have to use such a system. The trick a database system does, is that it holds the data in a file on the disk and operates on the data without loading all of it at once onto RAM. It can be slower, but it can processes pieces of information that do not fit in memory. Therefore, although it is not necessary to know how to built or administer such a system, one should know how use it. Since relational systems are most common we should examine them further. Some details of relational DBs The central concept is that of a relation. You can think of a relation like a spreadsheet table, that comprises of rows and columns. Each row has to be unique and is characterised by a key, while each column has to hold unique objects of the same type. Additional restrictions about the relations between the various columns of the table, ensure the validity of the theoretical model, and eventually that the database will operate as expected, and will not return incorrect results. SQLite in Python One simple and accessible database system is Sqlite. What makes it unique is that it has a simpler architecture for the features that offers, and a small size. That makes it ideal for small projects since the maintenance overhead is small. Python, through the pysqlite module of the standard library, offers a simple way to create and use a database with SQLite. For our purposes in this blog, this is the right tool. sources: Software Carpentry : SQL SQLite Python tutorial Back to top","tags":"3 Tools","url":"/tools-databases.html","loc":"/tools-databases.html"},{"title":"﻿Worked Example - 1 initial questions","text":"Previous article: Tools - Databases Next article: Worked Example - 2 get and store data Setting the stage for our project When we first talked about the setting stage of a project, we focused on the triangle that is being formed by the initial question, the relevant data, and the background/domain information. The main goal of this stage is to capture this triangle. In our project repository there is the phase_1 folder and inside there are two sub-folders named input and output, respectively. The names are pretty self-explanatory. In the input sub-folder we have collected all the relevant material along with a list of it; this is our initial inventory of material. The output folder contains three files: one on the initial question and motivation for the project, another on the relevant data, and a third that captures the background/domain information. All three files are the results of processing the material in the input sub-folder. Let us see in more detail how we came up with these documents. Input This sub-folder is the collection of all the relevant material that we need to begin with. In our case, the point of departure is the ASA poster competition, which provides us with the initial question: could we predict delays? Along with the question also come the relevant data that ASA published for the competition. In this way we have two out of three bases covered. The rest is about the background/domain information. The main focus is on flight delays. So, we gathered the posters that participated in the competition, along with other available web pages, papers, etc, that deal with the subject of the delays. This constitutes our input. Output The first file is about the initial question and the motivation. We combine the call of the ASA project, along with pieces of information about the problem flight delays cause to the US economy. Second comes the report on the relevant data. Since the initial project was a competition, it is not a surprise that the competition organisers provided for an adequate dataset. In our case, detailed data on flights and delays, along with extra pieces on information on individual planes, etc., are more than enough for the task at hand. Last comes the description of the domain of the airline industry, and a collection of stylised facts about flights and delays. The effort was to provide a small summary of the characteristics of delays, some explanations found in the literature, and make some points that might prove useful later in the project. In the relevant GitHub repository you can find all the this material and examine it in more detail. Sources: You can find the relevant material for this first phase at the GitHub project repository Back to top","tags":"4 Worked Example","url":"/worked-example-1-initial-questions.html","loc":"/worked-example-1-initial-questions.html"},{"title":"﻿Worked Example - 2 get and store data","text":"Previous article: Worked Example - 1 initial questions Next article: Worked Example - 3 clean data; Get and store data The main goal for this stage is to get the relevant data and store them in a way that will be useful for the project duration. In the previous section we had identified what the relevant data is; in our project the competition organisers have provided it for us. Take the time to examine the contents of the relevant project folders, and experiment with them. The range of dates go from 1987 to 2008, where each file contains millions of observations. For practical reasons we will download only the 2008 data; it will be enough for now. Therefore the input folder contains the four files on flights, carriers, airports, and planes. The raw data should strictly be read only. The output folders contains the next list of elements: The data report. In there one can find a detailed description of choices we make with respect to storing the data. Why we choose a relational database, why SQLite, points of attention with respect the the characteristics of the data, etc. The database documentation. In there one will find the specifics about the way our data fits into a relational database system, which in our case in SQLite. What the tables stand for, their interrelations, etc. It is the fundamental blueprint for our database The database creation script. It contains the definitions of the tables, the column names and types, etc. We have chosen to include it as a separate file for two main reasons. It supplements the rest of the documentation, and at the same time any change to the database - column names, types, etc. - start from this script. The Jupyter Notebook. In there one will find all the actual code for this phase with which we set up the database, following the blueprints, populated it with the data, tuned it, etc. Last, but not least, the database itself which will be the main source of data for the next phases. One of the things to note is that in effect we have already performed our first data check. While creating the database we had to perform certain checks in order to immediately catch out problems created by the transformation of the csv files, or by discrepancies between the actual csv and the definitions of the data providers. With these checks we have verified that, at least, what we have in our database is in accordance with the definitions we had. Sources: You can find the relevant material for the second phase at the GitHub project repository For the coding details, take a look at the Jupyter Notebook Back to top","tags":"4 Worked Example","url":"/worked-example-2-get-and-store-data.html","loc":"/worked-example-2-get-and-store-data.html"},{"title":"﻿Worked Example - 3 clean data","text":"Previous article: Worked Example - 2 get and store data Next article: Worked Example - 4 drawing conclusions; Clean Validate and Summarise The aim of this section is to gain a deeper understanding of the data, which implies weeding out erroneous input, and summarising the data. In the previous section we have already started out with weeding out duplicate lines, and other data transformations. In this section, we drill deeper, and we screen the data, not only for syntactic conformity to rules - correct data types, etc, - but also for their meaning. The input is, obviously, the database we have prepared in the previous section. Pay attention to a subtle issue. We should not alter the data, if possible, at all. Suppose we find a row in a table that seems an outlier - a technical glitch. Instead of deleting it, we flag it. That is because, as the project goes on, these outliers might seem interesting for analysis. If we delete them we will never know. The desired output, conceptually, is a clean dataset and a good understanding of it. The main locus is the Jupyter Notebook, that contains all the different things we tried out. Then we also get out of the notebook what seems relevant and pass it on to a couple of documents that we use to document and communicate our work: A detailed data report, where we describe the data, explain how we cleaned it, its main features, etc. The cleaned data set; a transformed copy of the \"read only\" raw data. The code script for that section, that should be reading the data, fixing what is to be fixed, and producing the necessary plots and summary statistics tables to be used in the data report. You can find these in the folders of phase_3. In our project, since we work with a curated dataset, as we could have expected, the cleaning has been at a minimum, therefore we did not have to provide a new, cleaned, version of the data. This part of this, or any other, project, is the boring part. We have done mostly mundane tasks like checking for null values and ranges of values. Although this activity is seemingly trivial, it is the basis of the project. If you follow the Jupyter notebook and the data report you will see how we got to an understanding of the data, and dug out things that are not obvious. For example we identified a relation that holds between the different columns, which works like an integrity check for the data. Knowing that will be useful later in understanding why, for example, certain columns seem correlated. All these little pieces of understanding that we have collected sip in the next stages of the project and improve it. Sources: You can find the relevant material for the third phase at the GitHub project repository For the coding details, take a look at the Jupyter Notebook Back to top","tags":"4 Worked Example","url":"/worked-example-3-clean-data.html","loc":"/worked-example-3-clean-data.html"},{"title":"﻿Worked Example - 4 drawing conclusions","text":"Previous article: Worked Example - 3 clean data Next article: Worked Example - 5 communicate Drawing conclusions - discovering new things Up to this point we have mostly dealt with the background and the data, that led us to drawing preliminary conclusions. It is now time to put together a model which we believe describes the process that generates the data, and proceed with using the data in order to estimate the model. There are four main conceptual stages that we follow here, and that one will find in the modelling report. First comes the formulation of a conceptual model. We were given an initial question, we took a deeper look at the data, therefore by now we should have some ideas about what the target to explain should be, and what could possibly explain it. In our case we chose to focus on the LateAircraftDelay category in order to simplify our task - divide an conquer. In the modelling report one will find a first, simple conceptual model that we hoped could provide some insight. Then, we have to operationalise the more abstract ideas developed previously. For example, we thought that previous delays for an airplane, could help explain current delays. What exactly should this feature be? Should we be looking at the delays for only the previous flight, or maybe a longer time span. Should it be expressed in minutes, or a percentage of something? We answered this question and settled for the features that we thought could better capture the concepts The next step is to turn the above ideas into a concrete dataset. Write the queries and get the data out of the database, put everything together, take care of the data types and other details so that the finalised dataset is ready to fed to the algorithmic model we have chosen for our problem. Finally, we get to the estimation of the model. We have to choose the right parameters, fit the model to the data, and interpret the results. Is our model any good, have we learnt anything? input At this stage the input is the clean dataset produced previously. output The desired output for this section is: The modelling report. It is the main document that describes all the choices and results. A copy of the data set to use for the model A copy of the trained model The code script for this section If one takes the time to read the modelling report one will find detailed description of the actual project. There are two Jupyter Notebooks, one with the creation of the features and another with the estimation of the model. Take a look at them. Sources: You can find the relevant material for the fourth phase at the GitHub project repository For the preparation of the data and the new features created, take a look at the Jupyter Notebook As for the model trained, here is the Notebook Back to top","tags":"4 Worked Example","url":"/worked-example-4-drawing-conclusions.html","loc":"/worked-example-4-drawing-conclusions.html"},{"title":"﻿Worked Example - 5 communicate","text":"Previous article: Worked Example - 4 drawing conclusions; Next article: Variations: web scraping and text mining Instead of anything else, take a look at the final project report that provides a nice summary of the whole project. sources: You can find all the relevant material for the last phase at the GitHub project repository Back to top","tags":"4 Worked Example","url":"/worked-example-5-communicate.html","loc":"/worked-example-5-communicate.html"},{"title":"﻿Variations -  Web scraping and text mining","text":"Previous article: Worked Example - 5 communicate Introduction This piece of text accompanies the codebase of a mini project on web-scarping and text analysis. The mini project aims at providing a variation to the themes we have covered until now: First, instead of downloading a ready dataset, we scrap a web page and collect the data we want. Second, the main part of dataset is textual and we use different kinds of algorithms. Our mini project has three, conceptual, parts: The scraper that scrapes off the data from web pages The database that holds the scraped data And the analytics engine, where the data reveals its secrets... This page, that describes the project and discusses some of its technical aspects, along with comments inserted in the code, should give an understanding of how the code works and why it is written that way. The next three sections correspond to the three conceptual parts described previously, and we shall see each one in turn. Introducing Scrapy This is the part responsible for collecting the data from the web. These programs are usually called web-scrapers and the part that goes from a web page to the next, is also called a spider. The main principles of their operation are the following: Ask for and get a web page from the Internet Parse the content of the web page and get out the desired data Route the scraped data to a means of data persistence Repeat for as long as there are links to follow We are using a Python framework dedicated to web-crawling, named Scrapy (version 1.6.0). This framework hides away a lot of the details, making it easier to get started with crawling the web, yet at the same time is powerful enough to be used in industrial software projects. Scrapy was chosen over Selenium and BeautifulSoup, two other similar Python frameworks, mostly because it can handle large projects, while also being user friendly. For more details and the documentation of Scrapy visit the docs What lies at the heart of the Scrapy work-flow is the concept of a \"scrapy project\", that when initiated most of the necessary code is automatically created and organised in a folder and a couple of sub-folders; in our case these are found in \"scraping_project/wikipedia1\" and \"scraping_project/wikipedia2\". Here is a list of the main files that define a project: settings.py: it controls all the commonly used settings for a project, like the name of the project, the speed to send page requests to a site, etc. spiders/spider_name.py: the central piece that defines how the crawler will move from page to page, how it will parse the pages, etc. items.py: in that we define the structure of the container the spider will use to collect and save the parsed data pipelines.py: finally, this file controls the way the spider will send the \"item\" it has collected, to a file on disk, or a database, etc. Our mini project: the scraper In our case we have two small scrapy projects, with their spiders. The target to scrape is wikipedia. In particular, from a page with a list of companies in Italy, spider1 gets the name, industry, sector, headquarter location, foundation year, along with the url of the company's page in wikipedia. Spider2 then, uses the list of urls, visits these pages one by one, and scrapes all the text (relevant to the firm) from each page. All the pieces of data are being saved into a document database, and this brings us to the second part... Introducing TinyDB! Databases are used in order to store, process and retrieve data, in ways more complex and sophisticated than what a simple file system could do. By the term we mean the file where the data physically reside, along the software systems that we use in order to perform the actions we want. In our case we have chosen to use TinyDB! , a serverless document-oriented database. Let us see what these terms mean. Usually database systems are build around the concept of client-server architecture, where the application is build in two parts, that communicate using network protocols - even inside one machine. TinyDB! is a single application, which makes it easier to set up and use. Document databases, belong to the NoSQL kind. Document stands for a record the database that can have many fields, that can also be nested. That means, that we can store data of arbitrary complexity in such a system, while it is also possible that each record has a different structure. In contrast, relational databases need a predefined structure, and can have trouble handling heterogeneous data. For our purposes a document database system with its flexibility is more suitable to hold scraped data. Note for example, that our two spiders, got different kinds of data from the pages they visited; with as relational database system we would have to declare in advance, once for each spider, the structure of the data. Additionally, TinyDB! has the advantage of being native to Python and can be used with a simple Python command. Of course, for a larger project, a more sophisticated system, such as MongoDB, should be used. Our mini project: the database For our project, the database is represented by the two json files found in the codebase folder. Each file is created and populated with data, by each of our two spiders. wikipeida1_db.json holds the data from the list of Italian firms, name, industry, sector, headquarter location, foundation year, along with the url of the company's page in wikipedia. wikipeida2_db.json holds the url along all the text of the company's wikipedia site. As a final step, we join the site text, to the rest of the company's data, and we get our main dataset that we will use for analysis... The analytics part Once we have a dataset, we can start with the analytics part. Of course, in practice even before a single line of code gets written, we do have an idea of what are we looking for. These questions determine, to a large extent, the data we are after and the techniques to employ for its analysis. In this mini project, we want to collect textual data and perform any kind of text data analysis with it. The main tool we are going to use is the Jupyter Notebook, an interactive, web based application that can run Python code, generate graphs and share the results of the analysis. The work flow of an analytics project, once the cleaned data set is in place, has roughly two phases. First we explore the data in order to gain an initial understanding about it, and then we build the model, statistical, or machine learning, etc, that we believe can answer the question we have. For this part we have made use of the Python scientific computing ecosystem, that includes NumPy for numerical computations, Pandas for handling and wrangling data, NLTK for the processing of out textual data, and SciPy for the algorithms we used for building our data model. What we have found out The figure below is a small example of the process of turning data into insights. What we see is the number of currently existing firms founded in Italy, per year, since 1750. Out of the existing main Italian companies, we have counted how may were founded for each year, since 1750. The relevant Wikipedia page contains a table with data for each firm. We managed to automatically get the data, store it, manipulate it, and transform the raw data into insights. This graph, for example, gives us a quick view of an aspect of the economic history of Italy. There was a boom in the birth of firms right after WWII, a slowdown during the 80's that was a time of economic crisis for Italy, etc. We should keep in mind that once we have the infrastructure ready, we can easily get much more data and insights. Wikipedia has pages listing the firms for many coutries, while our scraper, database, and analytics tools can capture and handle all that data. That is quite something indeed, isn't it? Sources: You can find all the relevant material for this mini project, at the GitHub repository Take a look, too, at all the code, in the Jupyter Notebook Back to top","tags":"5 Variations","url":"/variations-web-scraping-and-text-mining.html","loc":"/variations-web-scraping-and-text-mining.html"}]}